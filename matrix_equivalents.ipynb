{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Equivalents from the Hypercomplex Algebra\n",
    "\n",
    "Two numbers from a hypercomplex algebra $z, w \\in \\mathbb{H}^N$ can be multiplied such that the result $m = zw, m \\in \\mathbb{H}^N$ is an element of the same algebra. The space is closed under addition as well, and so hypercomplex algebras are also vector spaces.\n",
    "\n",
    "The map between an elementwise product of two hypercomplex vector  $\\hat{z}, \\hat{w} \\in \\mathbb{H}^{M \\times N}$ can be viewed as as a linear transformation on the elements of the right factor of the product $\\hat{m} = \\hat{z} \\hat{w} = A \\hat{w}$.\n",
    "\n",
    "Here the matrix $A$ depends on the entries of the geometric vector $\\hat{z}$. In this notebook, we explore a systematic way to generate these transformation matrices.\n",
    "\n",
    "$\\hat{z}, \\hat{w} \\in \\mathbb{H}^{M \\times N}$\n",
    "\n",
    "$\\hat{m} \\in \\mathbb{H}^{M \\times N}, \\hat{z} \\hat{w} = \\hat{m} $\n",
    "\n",
    "frame as a linear transformation with matrix $A_{[\\hat{z}]}$ conitioned on $\\hat{z}$\n",
    "\n",
    "$ A_{[\\hat{z}]} \\hat{w} = \\hat{m} $ \n",
    "\n",
    "estimate $A_{[\\hat{z}]}$ given $\\hat{w}, \\hat{m}$ with a static $\\hat{z}$ by randomly sampling $\\hat{w}$ and computing $\\hat{m}$\n",
    "\n",
    "$ A_{[\\hat{z}]}[t] \\in \\mathbb{R}^{N \\times N}, A_{[\\hat{z}]}[t + 1] = (\\hat{y} \\hat{x}^{T} + \\gamma A_{[\\hat{z}]}[t])(\\hat{x} \\hat{x}^{T} + \\gamma I)^{-1} $\n",
    "\n",
    "reduce equation in terms of $A_{[\\hat{z}]}[0]$\n",
    "\n",
    "$ A_{[\\hat{z}]}[t] = \\gamma^{t} A_{[\\hat{z}]}[0] (\\hat{x} \\hat{x}^{T} + \\gamma I)^{-t} + \\sum_{i = 1}^{t} \\gamma^{i - 1} \\hat{y} \\hat{x}^{T} (\\hat{x} \\hat{x}^{T} + \\gamma I)^{-i} $\n",
    "\n",
    "assume $ |\\gamma| < 1 $ and take limit $ t \\rightarrow \\infty $\n",
    "\n",
    "$ A_{[\\hat{z}]}[t \\rightarrow \\infty] = \\sum_{i = 1}^{\\infty} \\gamma^{i - 1} \\hat{y} \\hat{x}^{T} (\\hat{x} \\hat{x}^{T} + \\gamma I)^{-i} $\n",
    "\n",
    "does not depend on the initial choice of $A_{[\\hat{z}]}[0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypercomplex_conjugate(a):\n",
    "    c = np.ones(a.shape)\n",
    "    c[..., 1:] *= -1\n",
    "    return c * a\n",
    "def hypercomplex_multiply(a, b):\n",
    "    if a.shape[-1] == 1:\n",
    "        return a * b\n",
    "    else:\n",
    "        def cayley_dickson(p, q, r, s):\n",
    "            return np.concatenate([\n",
    "                (hypercomplex_multiply(\n",
    "                    p,\n",
    "                    r) -\n",
    "                hypercomplex_multiply(\n",
    "                    hypercomplex_conjugate(s),\n",
    "                    q)),\n",
    "                (hypercomplex_multiply(\n",
    "                    s,\n",
    "                    p) +\n",
    "                hypercomplex_multiply(\n",
    "                    q,\n",
    "                    hypercomplex_conjugate(r))),\n",
    "            ], axis=(len(a.shape) - 1))\n",
    "        return cayley_dickson(\n",
    "            a[..., :(a.shape[-1] // 2)],\n",
    "            a[..., (a.shape[-1] // 2):],\n",
    "            b[..., :(a.shape[-1] // 2)],\n",
    "            b[..., (a.shape[-1] // 2):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypercomplex_conjugate_gradient(a, da):\n",
    "    return hypercomplex_conjugate(da)   \n",
    "def hypercomplex_multiply_gradient(a, b, da, db):\n",
    "    return (hypercomplex_multiply(da, b),\n",
    "        hypercomplex_multiply(a, db))\n",
    "def hypercomplex_basis_gradient(a):\n",
    "    basis = np.zeros((a.shape[-1], a.shape[-1]))\n",
    "    np.fill_diagonal(basis, 1)\n",
    "    basis = basis.reshape((1, a.shape[-1], a.shape[-1]))\n",
    "    return basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HCX(object):\n",
    "    def random(*kdims, hcx_size=1, mean=0, std=1):\n",
    "        shape = [\n",
    "            kdims[i] if i < len(kdims) \n",
    "            else 2**hcx_size\n",
    "            for i in range(len(kdims) + 1)]\n",
    "        return np.random.normal(mean, std, shape)\n",
    "    def basis(x, dx=0, dir=1):\n",
    "        if dir > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return hypercomplex_basis_gradient(x)\n",
    "    def conj(x, dx=0, dir=1):\n",
    "        if dir > 0:\n",
    "            return hypercomplex_conjugate(x)\n",
    "        else:\n",
    "            return hypercomplex_conjugate_gradient(x, dx)\n",
    "    def add(x, y, dx=0, dy=0, dir=1):\n",
    "        if dir > 0:\n",
    "            return x + y\n",
    "        else:\n",
    "            return dx, dy\n",
    "    def sub(x, y, dx=0, dy=0, dir=1):\n",
    "        if dir > 0:\n",
    "            return x - y\n",
    "        else:\n",
    "            return dx, -dy\n",
    "    def mul(x, y, dx=0, dy=0, dir=1):\n",
    "        if dir > 0:\n",
    "            return hypercomplex_multiply(x, y)\n",
    "        else:\n",
    "            return hypercomplex_multiply_gradient(x, y, dx, dy)\n",
    "    def norm(x, dx=0, dir=1):\n",
    "        if dir > 0:\n",
    "            return np.sum(\n",
    "                hypercomplex_multiply(HCX.conj(x), x),\n",
    "                axis=(len(x.shape) - 1))**0.5\n",
    "        else:\n",
    "            c = 0.5 / np.sum(\n",
    "                hypercomplex_multiply(HCX.conj(x), x),\n",
    "                axis=(x.shape[-1] - 1))**0.5\n",
    "            g = hypercomplex_multiply_gradient(\n",
    "                HCX.conj(x), \n",
    "                x, \n",
    "                HCX.conj(x, dx, dir=-1), \n",
    "                dx)\n",
    "            r = HCX.conj(g[0]) + g[1]\n",
    "            return c * r\n",
    "    def inv(x, dx=0, dir=1):\n",
    "        if dir > 0:\n",
    "            return HCX.conj(x) / np.reshape(HCX.norm(x)**2, (-1, 1))\n",
    "        else:\n",
    "            return (HCX.conj(x, dx, dir=-1) \n",
    "                / HCX.norm(x)**2 \n",
    "                - 2 * HCX.conj(x) \n",
    "                / HCX.norm(x)**3\n",
    "                * HCX.norm(x, dx, dir=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.38828939682e-14\n",
      "Validation Loss: 3.58410744852e-27\n",
      "Iterations until Convergence: 200\n",
      "[[[ 2.05103854 -2.41028615 -0.54923662  1.17618732]\n",
      "  [ 2.41028615  2.05103854  1.17618732  0.54923662]\n",
      "  [ 0.54923662 -1.17618732  2.05103854 -2.41028615]\n",
      "  [-1.17618732 -0.54923662  2.41028615  2.05103854]]]\n"
     ]
    }
   ],
   "source": [
    "M = 2     # The hypercomplex size\n",
    "N = 1   # The vector elements\n",
    "T = 1e-20 # A convergenece threshold\n",
    "L = 0.05  # A hyperparameter to tune\n",
    "V = 100   # The number of validation steps\n",
    "D = 100   # The number of iterations before validating\n",
    "\n",
    "\n",
    "def validate(_a, _A):\n",
    "    _loss = 0.0\n",
    "    for i in range(V):\n",
    "        _b = HCX.random(N, 1, hcx_size=M).transpose(0, 2, 1)\n",
    "        _c = HCX.mul(\n",
    "            _a.transpose(0, 2, 1), \n",
    "            _b.transpose(0, 2, 1)).transpose(0, 2, 1)\n",
    "        _loss += np.sum(_c - np.matmul(_A, _b))**2\n",
    "    return _loss / V\n",
    "\n",
    "\n",
    "a = HCX.random(N, 1, hcx_size=M).transpose(0, 2, 1)\n",
    "A = HCX.random(N, 2**M, hcx_size=M)\n",
    "I = np.reshape(np.eye(2**M), (1, 2**M, 2**M))\n",
    "\n",
    "\n",
    "loss = 1.0\n",
    "iterations = 0\n",
    "while loss > T:\n",
    "    iterations += 1\n",
    "    b = HCX.random(N, 1, hcx_size=M).transpose(0, 2, 1)\n",
    "    c = HCX.mul(\n",
    "        a.transpose(0, 2, 1), \n",
    "        b.transpose(0, 2, 1)).transpose(0, 2, 1)\n",
    "    A = np.matmul(\n",
    "        (c * b.transpose(0, 2, 1)) + L * A,\n",
    "        np.linalg.inv(\n",
    "            (b * b.transpose(0, 2, 1)) + L * I))\n",
    "    if iterations % D == 0:\n",
    "        loss = validate(a, A)\n",
    "        print(\"Validation Loss:\", loss)\n",
    "    \n",
    "\n",
    "print(\"Iterations until Convergence:\", iterations)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 136.92079293]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.det(A)) # This matrix is full rank\n",
    "eigen_values, eigen_vectors = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.05103854+2.73761882j  2.05103854-2.73761882j  2.05103854+2.73761882j\n",
      "   2.05103854-2.73761882j]]\n"
     ]
    }
   ],
   "source": [
    "print(eigen_values) # The eigenvalues have a particular structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.03779313-0.64304569j -0.03779313+0.64304569j  0.20670594-0.48050617j\n",
      "    0.20670594+0.48050617j]\n",
      "  [-0.70039117+0.j         -0.70039117-0.j         -0.55639543+0.j\n",
      "   -0.55639543-0.j        ]\n",
      "  [ 0.04002650-0.22277876j  0.04002650+0.22277876j -0.33940515-0.43924391j\n",
      "   -0.33940515+0.43924391j]\n",
      "  [ 0.08013593-0.19199444j  0.08013593+0.19199444j -0.18028012+0.27600471j\n",
      "   -0.18028012-0.27600471j]]]\n"
     ]
    }
   ],
   "source": [
    "print(eigen_vectors) # The eigenvectors have a particular structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
